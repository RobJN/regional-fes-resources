{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# About\n",
    "\n",
    "The script reads the Electricity Demand components from the ETYS Spatial CSV file created by running SAS script \"Write_ETYS-Demands-CSV.sas\". It processes it to produce the CSV files required for the visualisation map."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import all libraries and set root driectory location\n",
    "import shutil, os, json, pandas as pd\n",
    "os.chdir(\"C:/Users/Rob.Nickerson/Desktop/regional-fes-resources-master\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: .\\ETYS data\\Output : The process cannot access the file because it is being used by another process\n",
      "WARNING: Creation of the directory .\\ETYS data\\Output failed\n"
     ]
    }
   ],
   "source": [
    "#Delete the outputs folder and all it's content (to remove any old data).\n",
    "dir_path = r\".\\ETYS data\\Output\"\n",
    "\n",
    "output_folders = [\"Active\" , \"DG\", \"DSR\" ,\"Sub1MW\"]\n",
    "            \n",
    "try:\n",
    "    shutil.rmtree(dir_path)\n",
    "except OSError as e:\n",
    "    print(\"Error: %s : %s\" % (dir_path, e.strerror))\n",
    "\n",
    "# We create a new empty Output folder.\n",
    "try:\n",
    "    os.mkdir(dir_path)\n",
    "except OSError:\n",
    "    print (\"WARNING: Creation of the directory %s failed\" % dir_path)\n",
    "else:\n",
    "    print (\"Successfully created the directory %s \" % dir_path)\n",
    "    for items in output_folders:\n",
    "        path = os.path.join(dir_path, items)\n",
    "        os.mkdir(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get list of regions in the spatail visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Region</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>ABHA1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>ABNE_P</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>ABTH_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>ACTL_2;CBNK_H;GREE_H;PERI_H</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>ALNE_P</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>312</td>\n",
       "      <td>WOHI_P</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>313</td>\n",
       "      <td>WTHU31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>314</td>\n",
       "      <td>WWEY_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>315</td>\n",
       "      <td>WYLF_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>316</td>\n",
       "      <td>WYMOM_1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>317 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Region\n",
       "0                          ABHA1\n",
       "1                         ABNE_P\n",
       "2                         ABTH_1\n",
       "3    ACTL_2;CBNK_H;GREE_H;PERI_H\n",
       "4                         ALNE_P\n",
       "..                           ...\n",
       "312                       WOHI_P\n",
       "313                       WTHU31\n",
       "314                       WWEY_1\n",
       "315                       WYLF_1\n",
       "316                      WYMOM_1\n",
       "\n",
       "[317 rows x 1 columns]"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Get the list of Regions in the spatial visualisation\n",
    "with open(r\".\\Geographies\\GSPs 2019\\GSP_post.geojson\") as f:\n",
    "    data = json.load(f) \n",
    "\n",
    "fin = open(\"./ETYS data/GSPs_VisualisationList.csv\", \"wt\")\n",
    "fin.write(\"Region\")\n",
    "fin.write('\\n')\n",
    "\n",
    "for feature in data['features']:\n",
    "    fin.write(feature['properties']['GSP(s)'])\n",
    "    fin.write('\\n')\n",
    "    \n",
    "fin.close()\n",
    "\n",
    "#Read resultant CSV in as GSP_Regions dataframe\n",
    "GSP_regions = pd.read_csv(r\".\\ETYS data\\GSPs_VisualisationList.csv\")\n",
    "\n",
    "GSP_regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>ABHA1</td>\n",
       "      <td>ABHA1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>ABNE_P</td>\n",
       "      <td>ABNE_P</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>ABTH_1</td>\n",
       "      <td>ABTH_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>ACTL_2</td>\n",
       "      <td>ACTL_2;CBNK_H;GREE_H;PERI_H</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>CBNK_H</td>\n",
       "      <td>ACTL_2;CBNK_H;GREE_H;PERI_H</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>354</td>\n",
       "      <td>WOHI_P</td>\n",
       "      <td>WOHI_P</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>355</td>\n",
       "      <td>WTHU31</td>\n",
       "      <td>WTHU31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>356</td>\n",
       "      <td>WWEY_1</td>\n",
       "      <td>WWEY_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>357</td>\n",
       "      <td>WYLF_1</td>\n",
       "      <td>WYLF_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>358</td>\n",
       "      <td>WYMOM_1</td>\n",
       "      <td>WYMOM_1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>359 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       index                            0\n",
       "0      ABHA1                        ABHA1\n",
       "1     ABNE_P                       ABNE_P\n",
       "2     ABTH_1                       ABTH_1\n",
       "3     ACTL_2  ACTL_2;CBNK_H;GREE_H;PERI_H\n",
       "4     CBNK_H  ACTL_2;CBNK_H;GREE_H;PERI_H\n",
       "..       ...                          ...\n",
       "354   WOHI_P                       WOHI_P\n",
       "355   WTHU31                       WTHU31\n",
       "356   WWEY_1                       WWEY_1\n",
       "357   WYLF_1                       WYLF_1\n",
       "358  WYMOM_1                      WYMOM_1\n",
       "\n",
       "[359 rows x 2 columns]"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#explode GSP_regions into grouped_regions\n",
    "\n",
    "grouped_regions = pd.concat([pd.Series(row['Region'], row['Region'].split(';'))              \n",
    "                    for _, row in GSP_regions.iterrows()]).reset_index()    \n",
    "\n",
    "grouped_regions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read input data CSVs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We drop un-used columns as soon as possible. This prevents us from using them in the future without first considering what corrections need applying to them first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  scenario    GSP  DemandPk type  year\n",
      "0       SP  ABHA1    70.149    C    19\n",
      "1       SP  ABHA1    70.912    C    20\n",
      "2       SP  ABHA1    71.497    C    21\n",
      "3       SP  ABHA1    71.974    C    22\n",
      "4       SP  ABHA1    72.314    C    23\n",
      "5       SP  ABHA1    72.688    C    24\n",
      "6       SP  ABHA1    72.970    C    25\n",
      "7       SP  ABHA1    73.172    C    26\n",
      "8       SP  ABHA1    73.399    C    27\n",
      "9       SP  ABHA1    73.573    C    28\n",
      "  scenario   tech  year etys_location  capacity\n",
      "0       CF  Hydro    20        ALNE_P      4.00\n",
      "1       CF  Hydro    20        ARDK_P      5.20\n",
      "2       CF  Hydro    20        ARMO_P      1.20\n",
      "3       CF  Hydro    20        BEAU_P      1.99\n",
      "4       CF  Hydro    20        BOAG_P      6.95\n",
      "5       CF  Hydro    20        BRAC_P      3.00\n",
      "6       CF  Hydro    20        BROA_P      1.20\n",
      "7       CF  Hydro    20        CAAD_P      2.40\n",
      "8       CF  Hydro    20          CAFA     12.00\n",
      "9       CF  Hydro    20        CASS_P      4.50\n",
      "  scenario     tech  year etys_location  capacity\n",
      "0       CF  Battery    20         ABHA1     0.134\n",
      "1       CF  Battery    20        ABNE_P     0.012\n",
      "2       CF  Battery    20        ABTH_1     0.027\n",
      "3       CF  Battery    20        ACTL_2     0.007\n",
      "4       CF  Battery    20        ALNE_P     0.004\n",
      "5       CF  Battery    20         ALVE1     0.115\n",
      "6       CF  Battery    20        AMEM_1     0.013\n",
      "7       CF  Battery    20        ARBR_P     0.012\n",
      "8       CF  Battery    20        ARMO_P     0.003\n",
      "9       CF  Battery    20         AXMI1     0.066\n",
      "  scenario    GSP    DSR  year\n",
      "0       SP  ABHA1 -4.347    20\n",
      "1       SP  ABHA1 -4.574    21\n",
      "2       SP  ABHA1 -4.795    22\n",
      "3       SP  ABHA1 -5.013    23\n",
      "4       SP  ABHA1 -5.213    24\n",
      "5       SP  ABHA1 -5.517    25\n",
      "6       SP  ABHA1 -5.843    26\n",
      "7       SP  ABHA1 -6.178    27\n",
      "8       SP  ABHA1 -6.721    28\n",
      "9       SP  ABHA1 -7.369    29\n"
     ]
    }
   ],
   "source": [
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "\n",
    "pd.options.mode.chained_assignment = \"warn\"\n",
    "\n",
    "# Read in the CSVs to dataframes\n",
    "df_active_csv = pd.read_csv(r\".\\ETYS data\\Input\\active.csv\")\n",
    "df_active_csv = df_active_csv.drop(columns=['DemandAM', 'DemandPM'])\n",
    "print(df_active_csv.head(10))\n",
    "\n",
    "df_DG_csv = pd.read_csv(r\".\\ETYS data\\Input\\DG.csv\")\n",
    "df_DG_csv = df_DG_csv.drop(columns=['wintpk', 'summam', 'summpm'])\n",
    "print(df_DG_csv.head(10))\n",
    "\n",
    "df_Sub1MW_csv = pd.read_csv(r\".\\ETYS data\\Input\\Sub1MW.csv\")\n",
    "df_Sub1MW_csv = df_Sub1MW_csv.drop(columns=['wintpk', 'summam', 'summpm'])\n",
    "print(df_Sub1MW_csv.head(10))\n",
    "\n",
    "df_DSR_csv = pd.read_csv(r\".\\ETYS data\\Input\\DSR.csv\")\n",
    "print(df_DSR_csv.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply corrections\n",
    "The ETYS spatial demand data has some edge case changes that we need to reverse out. These are:\n",
    "\n",
    "- Several Scottish GSPs are split in the ETYS data. These are the \"G_EXTRA\" locations and we will need to set them back to their Elexon registered GSP for the data visualisation.\n",
    "- There are cases of new GSPs added in the future taking a proportion of demand from other (existing) GSPs. As our visualisation does not show future new GSPs, we need to reverse the logic and add the demands back to their present GSP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Active\n",
    "df_active = df_active_csv.copy()\n",
    "df_active.loc[(df_active.GSP == 'G_EXTRA_1'),'GSP']='DUMF'\n",
    "df_active.loc[(df_active.GSP == 'G_EXTRA_2'),'GSP']='DUMF'\n",
    "df_active.loc[(df_active.GSP == 'G_EXTRA_3'),'GSP']='DUMF'\n",
    "df_active.loc[(df_active.GSP == 'G_EXTRA_4'),'GSP']='GRMO'\n",
    "df_active.loc[(df_active.GSP == 'G_EXTRA_5'),'GSP']='GRMO'\n",
    "df_active.loc[(df_active.GSP == 'G_EXTRA_6'),'GSP']='KILB'\n",
    "df_active.loc[(df_active.GSP == 'G_EXTRA_7'),'GSP']='KILB'\n",
    "df_active.loc[(df_active.GSP == 'G_EXTRA_8'),'GSP']='SACO'\n",
    "df_active.loc[(df_active.GSP == 'G_EXTRA_9'),'GSP']='SACO'\n",
    "df_active.loc[(df_active.GSP == 'G_EXTRA_10'),'GSP']='CROO'\n",
    "df_active.loc[(df_active.GSP == 'G_EXTRA_11'),'GSP']='CROO'\n",
    "df_active.loc[(df_active.GSP == 'DUNB_A'),'GSP']='DUNB'\n",
    "df_active.loc[(df_active.GSP == 'DUNB_B'),'GSP']='DUNB'\n",
    "\n",
    "##Reverse out G_EXTRA_13\n",
    "df_active.loc[(df_active.GSP == 'KINT_P') & (df_active.year >= 21),'DemandPk'] = df_active.DemandPk / (1 - 0.1164)\n",
    "df_active.loc[(df_active.GSP == 'KEIT_P') & (df_active.year >= 21),'DemandPk'] = df_active.DemandPk / (1 - 0.14457)\n",
    "\n",
    "##Reverse out G_EXTRA_14\n",
    "#Nothing to do for FES 2021.\n",
    "\n",
    "##Reverse out G_EXTRA_15\n",
    "df_active.loc[(df_active.GSP == 'LINM') & (df_active.year >= 25),'DemandPk'] = df_active.DemandPk / (1 - 0.13)\n",
    "\n",
    "##Reverse out G_EXTRA_16\n",
    "df_active.loc[(df_active.GSP == 'CHAP') & (df_active.year >= 23),'DemandPk'] = df_active.DemandPk / (1 - 0.42)\n",
    "\n",
    "##Reverse out ISLI_1\n",
    "#Commented out as we have actually added ISLI_1 to the geojson making a (approx) assumption that it is splitting only from WHAM_1.\n",
    "#df_active.loc[(df_active.GSP == 'LODR_6') & (df_active.year >= 22),'DemandPk'] = df_active.DemandPk / (1 - 0.25)\n",
    "#df_active.loc[(df_active.GSP == 'WHAM_1') & (df_active.year >= 22),'DemandPk'] = df_active.DemandPk / (1 - 0.45)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DG\n",
    "df_DG = df_DG_csv.copy()\n",
    "df_DG.loc[(df_DG.etys_location == 'G_EXTRA_1'),'etys_location']='DUMF'\n",
    "df_DG.loc[(df_DG.etys_location == 'G_EXTRA_2'),'etys_location']='DUMF'\n",
    "df_DG.loc[(df_DG.etys_location == 'G_EXTRA_3'),'etys_location']='DUMF'\n",
    "df_DG.loc[(df_DG.etys_location == 'G_EXTRA_4'),'etys_location']='GRMO'\n",
    "df_DG.loc[(df_DG.etys_location == 'G_EXTRA_5'),'etys_location']='GRMO'\n",
    "df_DG.loc[(df_DG.etys_location == 'G_EXTRA_6'),'etys_location']='KILB'\n",
    "df_DG.loc[(df_DG.etys_location == 'G_EXTRA_7'),'etys_location']='KILB'\n",
    "df_DG.loc[(df_DG.etys_location == 'G_EXTRA_8'),'etys_location']='SACO'\n",
    "df_DG.loc[(df_DG.etys_location == 'G_EXTRA_9'),'etys_location']='SACO'\n",
    "df_DG.loc[(df_DG.etys_location == 'G_EXTRA_10'),'etys_location']='CROO'\n",
    "df_DG.loc[(df_DG.etys_location == 'G_EXTRA_11'),'etys_location']='CROO'\n",
    "df_DG.loc[(df_DG.etys_location == 'DUNB_A'),'etys_location']='DUNB'\n",
    "df_DG.loc[(df_DG.etys_location == 'DUNB_B'),'etys_location']='DUNB'\n",
    "\n",
    "##Reverse out others\n",
    "#We don't have the ability to do that at this point in the analysis as we have aggregated technology types together.\n",
    "#TODO: Look at our upstream processes and change them so that we produce the visualisation data at that stage.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sub1MW\n",
    "df_Sub1MW = df_Sub1MW_csv.copy()\n",
    "df_Sub1MW.loc[(df_Sub1MW.etys_location == 'G_EXTRA_1'),'etys_location']='DUMF'\n",
    "df_Sub1MW.loc[(df_Sub1MW.etys_location == 'G_EXTRA_2'),'etys_location']='DUMF'\n",
    "df_Sub1MW.loc[(df_Sub1MW.etys_location == 'G_EXTRA_3'),'etys_location']='DUMF'\n",
    "df_Sub1MW.loc[(df_Sub1MW.etys_location == 'G_EXTRA_4'),'etys_location']='GRMO'\n",
    "df_Sub1MW.loc[(df_Sub1MW.etys_location == 'G_EXTRA_5'),'etys_location']='GRMO'\n",
    "df_Sub1MW.loc[(df_Sub1MW.etys_location == 'G_EXTRA_6'),'etys_location']='KILB'\n",
    "df_Sub1MW.loc[(df_Sub1MW.etys_location == 'G_EXTRA_7'),'etys_location']='KILB'\n",
    "df_Sub1MW.loc[(df_Sub1MW.etys_location == 'G_EXTRA_8'),'etys_location']='SACO'\n",
    "df_Sub1MW.loc[(df_Sub1MW.etys_location == 'G_EXTRA_9'),'etys_location']='SACO'\n",
    "df_Sub1MW.loc[(df_Sub1MW.etys_location == 'G_EXTRA_10'),'etys_location']='CROO'\n",
    "df_Sub1MW.loc[(df_Sub1MW.etys_location == 'G_EXTRA_11'),'etys_location']='CROO'\n",
    "df_Sub1MW.loc[(df_Sub1MW.etys_location == 'DUNB_A'),'etys_location']='DUNB'\n",
    "df_Sub1MW.loc[(df_Sub1MW.etys_location == 'DUNB_B'),'etys_location']='DUNB'\n",
    "\n",
    "##Reverse out others\n",
    "#We don't have the ability to do that at this point in the analysis as we have aggregated technology types together.\n",
    "#TODO: Look at our upstream processes and change them so that we produce the visualisation data at that stage.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DSR\n",
    "df_DSR = df_DSR_csv.copy()\n",
    "df_DSR.loc[(df_DSR.GSP == 'G_EXTRA_1'),'GSP']='DUMF'\n",
    "df_DSR.loc[(df_DSR.GSP == 'G_EXTRA_2'),'GSP']='DUMF'\n",
    "df_DSR.loc[(df_DSR.GSP == 'G_EXTRA_3'),'GSP']='DUMF'\n",
    "df_DSR.loc[(df_DSR.GSP == 'G_EXTRA_4'),'GSP']='GRMO'\n",
    "df_DSR.loc[(df_DSR.GSP == 'G_EXTRA_5'),'GSP']='GRMO'\n",
    "df_DSR.loc[(df_DSR.GSP == 'G_EXTRA_6'),'GSP']='KILB'\n",
    "df_DSR.loc[(df_DSR.GSP == 'G_EXTRA_7'),'GSP']='KILB'\n",
    "df_DSR.loc[(df_DSR.GSP == 'G_EXTRA_8'),'GSP']='SACO'\n",
    "df_DSR.loc[(df_DSR.GSP == 'G_EXTRA_9'),'GSP']='SACO'\n",
    "df_DSR.loc[(df_DSR.GSP == 'G_EXTRA_10'),'GSP']='CROO'\n",
    "df_DSR.loc[(df_DSR.GSP == 'G_EXTRA_11'),'GSP']='CROO'\n",
    "df_DSR.loc[(df_DSR.GSP == 'DUNB_A'),'GSP']='DUNB'\n",
    "df_DSR.loc[(df_DSR.GSP == 'DUNB_B'),'GSP']='DUNB'\n",
    "\n",
    "##Reverse out others\n",
    "#TODO.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Produce outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "#Active.csv\n",
    "\n",
    "for category in [\"C\", \"D\", \"E\", \"H\", \"I\", \"R\", \"Z\"]:\n",
    "    for scenario in [\"SP\", \"CT\", \"ST\", \"LW\"]:\n",
    "        # Create df as a filter of df_csv\n",
    "        df = df_active[(df_active['scenario'] == scenario) & (df_active['type'] == category)]\n",
    "        df['year'] = df['year'] + 2000\n",
    "        \n",
    "        #Merge df on grouped_regions. Drop index & GSP columns\n",
    "        df = df.merge(grouped_regions, left_on = \"GSP\", right_on = \"index\", how = 'left').drop(columns = [\"index\" , \"GSP\"]).rename(columns = {0:\"Region\"})\n",
    "        \n",
    "        # Pivot to have years across the top\n",
    "        df = pd.pivot_table(df, index='Region', columns= 'year', values='DemandPk', aggfunc = 'sum')\n",
    "        \n",
    "        # Export to CSV\n",
    "        df.index.name = 'Primary'\n",
    "        filename = scenario + \"-DemandPk-\" + category + \".csv\"\n",
    "        df.to_csv(r\".\\ETYS data\\Output\\Active\\\\\" + filename, index=True, float_format='%.3f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "#Active.csv (Aggregated)\n",
    "\n",
    "for scenario in [\"SP\", \"CT\", \"ST\", \"LW\"]:\n",
    "    # Create df as a filter of df_csv\n",
    "    df = df_active[(df_active['scenario'] == scenario)]\n",
    "    df['year'] = df['year'] + 2000\n",
    "\n",
    "    #Merge df on grouped_regions. Drop index & GSP columns\n",
    "    df = df.merge(grouped_regions, left_on = \"GSP\", right_on = \"index\", how = 'left').drop(columns = [\"index\" , \"GSP\"]).rename(columns = {0:\"Region\"})\n",
    "\n",
    "    # Pivot to have years across the top\n",
    "    df = pd.pivot_table(df, index='Region', columns= 'year', values='DemandPk', aggfunc = 'sum')\n",
    "\n",
    "    # Export to CSV\n",
    "    df.index.name = 'Primary'\n",
    "    filename = scenario + \"-DemandPk-\" + \"All\" + \".csv\"\n",
    "    df.to_csv(r\".\\ETYS data\\Output\\Active\\\\\" + filename, index=True, float_format='%.3f')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "#DG.csv\n",
    "\n",
    "for technology in [\"Hydro\", \"Other\", \"Solar\", \"Storage\", \"Wind\"]:\n",
    "    for scenario in [\"SP\", \"CT\", \"ST\", \"LW\"]:\n",
    "        # Create df as a filter of df_csv\n",
    "        df = df_DG[(df_DG['scenario'] == scenario) & (df_DG['tech'] == technology)]\n",
    "        df['year'] = df['year'] + 2000\n",
    "           \n",
    "        #Merge df on grouped_regions. Drop index & etys_location columns\n",
    "        df = df.merge(grouped_regions, left_on = \"etys_location\", right_on = \"index\", how = 'left').drop(columns = [\"index\" , \"etys_location\"]).rename(columns = {0:\"Region\"})\n",
    "        \n",
    "        # Pivot to have years across the top\n",
    "        df = pd.pivot_table(df, index='Region', columns= 'year', values='capacity', aggfunc = 'sum')\n",
    "        \n",
    "        # Export to CSV\n",
    "        df.index.name = 'Primary'\n",
    "        filename = scenario + \"-DxCapacity-\" + technology + \".csv\"\n",
    "        df.to_csv(r\".\\ETYS data\\Output\\DG\\\\\" + filename, index=True, float_format='%.3f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "#Sub1MW.csv\n",
    "\n",
    "for technology in [\"Hydro\", \"Other\", \"Solar\", \"Battery\", \"Wind\"]:\n",
    "    for scenario in [\"SP\", \"CT\", \"ST\", \"LW\"]:\n",
    "        # Create df as a filter of df_csv\n",
    "        df = df_Sub1MW[(df_Sub1MW['scenario'] == scenario) & (df_Sub1MW['tech'] == technology)]\n",
    "        df['year'] = df['year'] + 2000\n",
    "        \n",
    "        #Merge df on grouped_regions. Drop index & GSP columns\n",
    "        df = df.merge(grouped_regions, left_on = \"etys_location\", right_on = \"index\", how = 'left').drop(columns = [\"index\" , \"etys_location\"]).rename(columns = {0:\"Region\"})\n",
    "        \n",
    "        # Pivot to have years across the top\n",
    "        df = pd.pivot_table(df, index='Region', columns= 'year', values='capacity', aggfunc = 'sum')\n",
    "        \n",
    "        # Export to CSV\n",
    "        df.index.name = 'Primary'\n",
    "        if technology == \"Battery\":\n",
    "            filename = scenario + \"-MxCapacity-\" + \"Storage\" + \".csv\"\n",
    "        else:\n",
    "            filename = scenario + \"-MxCapacity-\" + technology + \".csv\"\n",
    "        df.to_csv(r\".\\ETYS data\\Output\\Sub1MW\\\\\" + filename, index=True, float_format='%.3f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "#DSR.csv\n",
    "\n",
    "for scenario in [\"SP\", \"CT\", \"ST\", \"LW\"]:\n",
    "    # Create df as a filter of df_csv\n",
    "    df = df_DSR[(df_DSR['scenario'] == scenario)]\n",
    "    df['year'] = df['year'] + 2000\n",
    "\n",
    "    #Merge df on grouped_regions. Drop index & GSP columns\n",
    "    df = df.merge(grouped_regions, left_on = \"GSP\", right_on = \"index\", how = 'left').drop(columns = [\"index\" , \"GSP\"]).rename(columns = {0:\"Region\"})\n",
    "    \n",
    "    # Pivot to have years across the top\n",
    "    df = pd.pivot_table(df, index='Region', columns= 'year', values='DSR', aggfunc = 'sum')\n",
    "\n",
    "    # Export to CSV\n",
    "    df.index.name = 'Primary'\n",
    "    filename = scenario + \"-DSR-\" + \".csv\"\n",
    "    df.to_csv(r\".\\ETYS data\\Output\\DSR\\\\\" + filename, index=True, float_format='%.3f')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: The active dataset also includes transmission sites that we do not include in the Regional Visulisation. These start \"T_\", \"M_\" \"B_EXTRA\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
